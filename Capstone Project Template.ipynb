{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from us_state_abbrev import state_udf, abbrev_state, abbrev_state_udf,city_code_udf,city_codes\n",
    "from immigration_codes import country_udf\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Build spark session\n",
    "\"\"\"\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Build SQL context object\n",
    "\"\"\"\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Reading in the data\n",
    "\"\"\"\n",
    "demog=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "airport=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "temperatureData=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"GlobalLandTemperaturesByState.csv\")\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Performing cleaning tasks and Data Agregation\n",
    "  \n",
    "\"\"\"\n",
    "usTemperatures=temperatureData.filter(temperatureData[\"country\"]==\"United States\")\\\n",
    "    .filter(year(temperatureData[\"dt\"])==2013)\\\n",
    "    .withColumn(\"year\",year(temperatureData[\"dt\"]))\\\n",
    "    .withColumn(\"month\",month(temperatureData[\"dt\"]))\\\n",
    "    .withColumn(\"avg_temp_fahrenheit\",temperatureData[\"AverageTemperature\"]*9/5+32)\\\n",
    "    .withColumn(\"state_abbrev\",state_udf(temperatureData[\"State\"]))\n",
    "\n",
    "clean_Temperatures=usTemperatures.select(\"year\",\"month\",round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                                       round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\n",
    "                                       \"state_abbrev\",\"State\",\"Country\").dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|year|month|avg_temp_celcius|avg_temp_fahrenheit|state_abbrev|        State|      Country|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|2013|    7|            23.4|               74.1|          MA|Massachusetts|United States|\n",
      "|2013|    3|            -1.9|               28.5|          SD| South Dakota|United States|\n",
      "|2013|    9|            14.1|               57.4|          ME|        Maine|United States|\n",
      "|2013|    1|            -1.3|               29.7|          PA| Pennsylvania|United States|\n",
      "|2013|    9|            25.1|               77.2|          AL|      Alabama|United States|\n",
      "|2013|    9|            21.0|               69.8|          IL|     Illinois|United States|\n",
      "|2013|    3|            10.8|               51.4|          MS|  Mississippi|United States|\n",
      "|2013|    8|            20.5|               68.9|          RI| Rhode Island|United States|\n",
      "|2013|    5|            18.9|               66.0|          KY|     Kentucky|United States|\n",
      "|2013|    6|            25.4|               77.7|          AR|     Arkansas|United States|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_Temperatures.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  remove nulls then convert i94res codes to country of origin and filter out NULLS and run country_udf function to show state names\n",
    "  country_udf, abbrev_state_udf and city_code_udf were created with data from SAS labels Descriptions file.\n",
    "\n",
    "\"\"\"\n",
    "i94_data=df_spark.filter(df_spark.i94addr.isNotNull())\\\n",
    "    .filter(df_spark.i94res.isNotNull())\\\n",
    "    .filter(col(\"i94addr\").isin(list(abbrev_state.keys())))\\\n",
    "    .filter(col(\"i94port\").isin(list(city_codes.keys())))\\\n",
    "    .withColumn(\"origin_country\",country_udf(df_spark[\"i94res\"]))\\\n",
    "    .withColumn(\"dest_state_name\",abbrev_state_udf(df_spark[\"i94addr\"]))\\\n",
    "    .withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    "    .withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\"))\\\n",
    "    .withColumn(\"city_port_name\",city_code_udf(df_spark[\"i94port\"]))\n",
    "\n",
    "clean_I94_Data=i94_data.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"),\\\n",
    "                             \"origin_country\",\"i94port\",\"city_port_name\",col(\"i94addr\").alias(\"state_code\"),\"dest_state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------------+-------+--------------------+----------+---------------+\n",
      "|cicid|year|month|origin_country|i94port|      city_port_name|state_code|dest_state_name|\n",
      "+-----+----+-----+--------------+-------+--------------------+----------+---------------+\n",
      "|  7.0|2016|    4|   SOUTH KOREA|    ATL|  ATLANTA           |        AL|        Alabama|\n",
      "| 15.0|2016|    4|       ALBANIA|    WAS|WASHINGTON DC    ...|        MI|       Michigan|\n",
      "| 16.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        MA|  Massachusetts|\n",
      "| 17.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        MA|  Massachusetts|\n",
      "| 18.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        MI|       Michigan|\n",
      "| 19.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        NJ|     New Jersey|\n",
      "| 20.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        NJ|     New Jersey|\n",
      "| 21.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        NY|       New York|\n",
      "| 22.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        NY|       New York|\n",
      "| 23.0|2016|    4|       ALBANIA|    NYC|  NEW YORK          |        NY|       New York|\n",
      "+-----+----+-----+--------------+-------+--------------------+----------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_I94_Data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Calculate percentages of each numeric column and create new columns.\n",
    "\"\"\"\n",
    "us_demographic_data=demog\\\n",
    "    .withColumn(\"Median Age\",col(\"Median Age\").cast(\"float\"))\\\n",
    "    .withColumn(\"pct_male_pop\",demog[\"Male Population\"]/demog[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"pct_female_pop\",demog[\"Female Population\"]/demog[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"pct_veterans\",demog[\"Number of Veterans\"]/demog[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"pct_foreign_born\",demog[\"Foreign-born\"]/demog[\"Total Population\"]*100)\\\n",
    "    .withColumn(\"pct_race\",demog[\"Count\"]/demog[\"Total Population\"]*100)\\\n",
    "    .orderBy(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Select columns with new calculated percentages and state names.\n",
    "\"\"\"\n",
    "us_demographic_data_with_percentage=us_demographic_data.select(\"State\",col(\"State Code\").alias(\"state_code\"),\\\n",
    "     col(\"Median Age\").alias(\"median_age\"),\\\n",
    "     \"pct_male_pop\",\\\n",
    "     \"pct_female_pop\",\\\n",
    "     \"pct_veterans\",\\\n",
    "     \"pct_foreign_born\",\\\n",
    "     \"Race\",\\\n",
    "     \"pct_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Pivoting the Race column\n",
    "\"\"\"\n",
    "pivot_us_demographic_data=us_demographic_data_with_percentage.groupBy(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\\\n",
    "                                    \"pct_female_pop\",\"pct_veterans\",\\\n",
    "                                    \"pct_foreign_born\").pivot(\"Race\").avg(\"pct_race\")\n",
    "\n",
    "pivot_us_demographic_data=pivot_us_demographic_data.select(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\"pct_veterans\",\"pct_foreign_born\",\\\n",
    "                                         col(\"American Indian and Alaska Native\").alias(\"native_american\"),\\\n",
    "                                         col(\"Asian\"),col(\"Black or African-American\").alias(\"Black\"),\\\n",
    "                                         col(\"Hispanic or Latino\").alias(\"hispanic_or_latino\"),\"White\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Find the average of each column per state. \n",
    "\"\"\"\n",
    "pivot=pivot_us_demographic_data.groupBy(\"State\",\"state_code\").avg(\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\\\n",
    "                                                       \"pct_veterans\",\"pct_foreign_born\",\"native_american\",\\\n",
    "                                                       \"Asian\",\"Black\",\"hispanic_or_latino\",\"White\").orderBy(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Round the percentages and fix column names\n",
    "\"\"\"\n",
    "pivot=pivot.select(\"State\",\"state_code\",round(col(\"avg(median_age)\"),1).alias(\"median_age\"),\\\n",
    "                  round(col(\"avg(pct_male_pop)\"),1).alias(\"pct_male_pop\"),\\\n",
    "                   round(col(\"avg(pct_female_pop)\"),1).alias(\"pct_female_pop\"),\\\n",
    "                   round(col(\"avg(pct_veterans)\"),1).alias(\"pct_veterans\"),\\\n",
    "                   round(col(\"avg(pct_foreign_born)\"),1).alias(\"pct_foreign_born\"),\\\n",
    "                   round(col(\"avg(native_american)\"),1).alias(\"native_american\"),\\\n",
    "                   round(col(\"avg(Asian)\"),1).alias(\"Asian\"),\\\n",
    "                   round(col(\"avg(hispanic_or_latino)\"),1).alias(\"hispanic_or_latino\"),\\\n",
    "                   round(col(\"avg(Black)\"),1).alias(\"Black\"),\\\n",
    "                   round(col('avg(White)'),1).alias('White')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|               State|state_code|median_age|pct_male_pop|pct_female_pop|pct_veterans|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+--------------------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|             Alabama|        AL|      36.2|        47.2|          52.8|         6.8|             5.1|            0.8|  2.9|               3.6| 45.0| 52.0|\n",
      "|              Alaska|        AK|      32.2|        51.2|          48.8|         9.2|            11.1|           12.2| 12.3|               9.1|  7.7| 71.2|\n",
      "|             Arizona|        AZ|      35.0|        48.8|          51.2|         6.6|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "|            Arkansas|        AR|      32.8|        48.4|          51.6|         5.2|            10.7|            1.8|  4.1|              14.2| 21.8| 68.0|\n",
      "|          California|        CA|      36.2|        49.4|          50.6|         4.1|            27.6|            1.7| 17.9|              37.8|  7.5| 62.7|\n",
      "|            Colorado|        CO|      35.8|        49.4|          50.6|         6.2|             9.6|            2.0|  4.9|              22.2|  4.2| 88.0|\n",
      "|         Connecticut|        CT|      35.0|        48.9|          51.1|         2.9|            25.2|            1.3|  5.3|              34.8| 24.3| 59.6|\n",
      "|            Delaware|        DE|      36.4|        45.4|          54.6|         4.3|             4.6|            0.6|  1.7|               7.7| 61.4| 33.0|\n",
      "|District of Columbia|        DC|      33.8|        47.6|          52.4|         3.9|            14.1|            0.9|  5.2|              10.6| 48.9| 42.5|\n",
      "|             Florida|        FL|      39.7|        48.1|          51.9|         5.7|            24.9|            0.9|  4.0|              28.4| 23.5| 70.4|\n",
      "+--------------------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Filtering the airport data for 'small_airport' in the U.S. and use substring to show state\n",
    " Find average elevation per state, select relevant columns and drop duplicates.\n",
    "\"\"\"\n",
    "airport_data=airport.filter(airport[\"type\"]==\"small_airport\")\\\n",
    ".filter(airport[\"iso_country\"]==\"US\")\\\n",
    ".withColumn(\"iso_region\",substring(airport[\"iso_region\"],4,2))\\\n",
    ".withColumn(\"elevation_ft\",col(\"elevation_ft\").cast(\"float\"))\n",
    "\n",
    "\n",
    "airport_data_elevation=airport_data.groupBy(\"iso_country\",\"iso_region\").avg(\"elevation_ft\")\n",
    "\n",
    "\n",
    "clean_airport_data=airport_data_elevation.select(col(\"iso_country\").alias(\"country\"),\\\n",
    "                                               col(\"iso_region\").alias(\"state\"),\\\n",
    "                                               round(col(\"avg(elevation_ft)\"),1).alias(\"avg_elevation_ft\")).orderBy(\"iso_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------------+\n",
      "|country|state|avg_elevation_ft|\n",
      "+-------+-----+----------------+\n",
      "|     US|   AK|           545.1|\n",
      "|     US|   AL|           414.6|\n",
      "|     US|   AR|           488.4|\n",
      "|     US|   AZ|          3098.0|\n",
      "|     US|   CA|          1261.4|\n",
      "|     US|   CO|          5912.8|\n",
      "|     US|   CT|           490.3|\n",
      "|     US|   DE|            47.4|\n",
      "|     US|   FL|            77.7|\n",
      "|     US|   GA|           649.5|\n",
      "+-------+-----+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_airport_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Create Dimension tables and allow unlimited time for SQL joins and parquet writes.\n",
    "\"\"\"\n",
    "clean_I94_Data.createOrReplaceTempView(\"immigration\")\n",
    "pivot.createOrReplaceTempView(\"demographics\")\n",
    "clean_airport_data.createOrReplaceTempView(\"airport\")\n",
    "clean_Temperatures.createOrReplaceTempView(\"temperature\")\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Creating the fact table by joining to the dimension tables above and counting how many people immigrated to each state in the U.S.\n",
    "\"\"\"\n",
    "immigration_to_states=spark.sql(\"\"\"SELECT \n",
    "                                    m.year,\n",
    "                                    m.month AS immig_month,\n",
    "                                    m.origin_country AS immig_origin,\n",
    "                                    m.dest_state_name AS to_immig_state,\n",
    "                                    COUNT(m.state_code) AS to_immig_state_count,\n",
    "                                    t.avg_temp_fahrenheit,\n",
    "                                    a.avg_elevation_ft,\n",
    "                                    d.pct_foreign_born,\n",
    "                                    d.native_american,\n",
    "                                    d.Asian,\n",
    "                                    d.hispanic_or_latino,\n",
    "                                    d.Black,\n",
    "                                    d.White\n",
    "                                    \n",
    "                                    FROM immigration m JOIN temperature t ON m.state_code=t.state_abbrev AND m.month=t.month\n",
    "                                    JOIN demographics d ON d.state_code=t.state_abbrev\n",
    "                                    JOIN airport a ON a.state=t.state_abbrev\n",
    "                                    \n",
    "                                    GROUP BY m.year,m.month, m.origin_country,\\\n",
    "                                    m.dest_state_name,m.state_code,t.avg_temp_fahrenheit,a.avg_elevation_ft,\\\n",
    "                                    d.pct_foreign_born,d.native_american,\\\n",
    "                                    d.Asian,d.hispanic_or_latino,\\\n",
    "                                    d.hispanic_or_latino,d.White,\\\n",
    "                                    d.Black\n",
    "                                    \n",
    "                                    ORDER BY m.origin_country,m.state_code\n",
    "                                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|year|immig_month|immig_origin|to_immig_state|to_immig_state_count|avg_temp_fahrenheit|avg_elevation_ft|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|2016|          4| AFGHANISTAN|       Arizona|                   1|               59.3|          3098.0|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "|2016|          4| AFGHANISTAN|    California|                  34|               58.5|          1261.4|            27.6|            1.7| 17.9|              37.8|  7.5| 62.7|\n",
      "|2016|          4| AFGHANISTAN|      Colorado|                   3|               39.7|          5912.8|             9.6|            2.0|  4.9|              22.2|  4.2| 88.0|\n",
      "|2016|          4| AFGHANISTAN|      Delaware|                   1|               52.7|            47.4|             4.6|            0.6|  1.7|               7.7| 61.4| 33.0|\n",
      "|2016|          4| AFGHANISTAN|       Florida|                  26|               71.9|            77.7|            24.9|            0.9|  4.0|              28.4| 23.5| 70.4|\n",
      "|2016|          4| AFGHANISTAN|       Georgia|                   5|               63.8|           649.5|            10.3|            0.9|  6.5|               7.7| 41.5| 51.4|\n",
      "|2016|          4| AFGHANISTAN|        Hawaii|                   2|               71.1|           915.5|            28.7|            1.6| 68.3|               7.0|  3.3| 31.3|\n",
      "|2016|          4| AFGHANISTAN|      Illinois|                   1|               50.3|           686.6|            20.9|            0.8| 10.5|              22.8| 14.3| 66.8|\n",
      "|2016|          4| AFGHANISTAN|       Indiana|                   1|               51.2|           766.1|             7.4|            0.9|  4.9|               9.6| 21.7| 72.8|\n",
      "|2016|          4| AFGHANISTAN|        Kansas|                   1|               48.7|          1600.3|             9.8|            2.2|  5.6|              13.2| 11.7| 82.0|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_to_states.toDF('year', 'immig_month', 'immig_origin', 'to_immig_state', \\\n",
    "          'to_immig_state_count', 'avg_temp_fahrenheit', 'avg_elevation_ft',\\\n",
    "          'pct_foreign_born', 'native_american', 'Asian', 'hispanic_or_latino', 'Black', 'White').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Write fact table to parquet\n",
    "\"\"\"\n",
    "immigration_to_states.write.parquet(\"immigration_to_states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+\n",
      "| year|month|country|state|\n",
      "+-----+-----+-------+-----+\n",
      "|false|false|  false|false|\n",
      "+-----+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Perform quality checks here\n",
    " Check for NULL values in year, month, origin_country, to_immig_state\n",
    "\"\"\"\n",
    "immigration_to_states.select(isnull('year').alias('year'),\\\n",
    "                             isnull('immig_month').alias('month'),\\\n",
    "                             isnull('immig_origin').alias('country'),\\\n",
    "                             isnull('to_immig_state').alias('state')).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|fact_table_count|\n",
      "+----------------+\n",
      "|         2780777|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Counting Total number of emigrants to US from the fact table .\n",
    "\"\"\"\n",
    "immigration_to_states.select(sum('to_immig_state_count').alias('fact_table_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2783521|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " Count the total number of immigrants from the source data\n",
    "\"\"\"\n",
    "spark.sql('SELECT COUNT(*) FROM immigration').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Dictionary Dimension Tables**\n",
    "## Airport Data by State\n",
    " * country: string (nullable = true)-United States\n",
    " * state: string (nullable = true)-State of the U.S.\n",
    " * avg_elevation_ft: double (nullable = true)-Average elevation based on all airport data from each state.\n",
    " \n",
    "## U.S. Demographic by State\n",
    " * State: string (nullable = true)-Full state name\n",
    " * state_code: string (nullable = true)-Abbreviated state code\n",
    " * median_age: double (nullable = true)-Median Age per state\n",
    " * pct_male_pop: double (nullable = true)- % Avg Male population per state\n",
    " * pct_female_pop: double (nullable = true)-% Avg Female population per state\n",
    " * pct_veterans: double (nullable = true)-% Avg Veteran population per state\n",
    " * pct_foreign_born: double (nullable = true)-% Avg Foreign-Born population per state\n",
    " * native_american: double (nullable = true)-% Avg Native American population per state\n",
    " * Asian: double (nullable = true)-% Avg Asian population per state\n",
    " * hispanic_or_latino: double (nullable = true)% Avg Hispanic or Latino population per state\n",
    " * Black: double (nullable = true)-% Avg Black population per state\n",
    " * White: double (nullable = true)-% Avg White population per state\n",
    " \n",
    "## Immigration Data by State with Origin\n",
    " * cicid: double (nullable = true)-ID number of each individual\n",
    " * year: integer (nullable = true)-Year of Immigration\n",
    " * month: integer (nullable = true)-Month of Immigration\n",
    " * origin_country: string (nullable = true)-Country of Origin\n",
    " * i94port: string (nullable = true)-City Port Code where Immigrant entered\n",
    " * city_port_name: string (nullable = true)-City Port Name\n",
    " * state_code: string (nullable = true)-Abbreviated State code\n",
    " * dest_state_name: string (nullable = true)-State Name\n",
    "\n",
    "## Temperature Data by State\n",
    " * year: integer (nullable = true)-Temperature Year\n",
    " * month: integer (nullable = true)-Temerpature Month\n",
    " * avg_temp_celcius: double (nullable = true)-Avg Temperature in Celcius per State\n",
    " * avg_temp_fahrenheit: double (nullable = true)-Avg Temperatrue in Fahrenheit\n",
    " * state_abbrev: string (nullable = true)-Abbreviated State Code\n",
    " * State: string (nullable = true)-State Name\n",
    " * Country: string (nullable = true)-United States\n",
    "\n",
    "# Fact Table\n",
    " * year: integer (nullable = true)-Year from immigration table\n",
    " * immig_month: integer (nullable = true)-Month from immigration table\n",
    " * immig_origin: string (nullable = true)-Country of Origin from immigration table\n",
    " * to_immig_state: string (nullable = true)-State immigrated to from immigration table\n",
    " * to_immig_state_count: long (nullable = false)-Total count of people immigrated per state from immigration table\n",
    " * avg_temp_fahrenheit: double (nullable = true)-Avg temperature per state from Temperature table\n",
    " * avg_elevation_ft: double (nullable = true)-Avg elevation per state from Airport table\n",
    " * pct_foreign_born: double (nullable = true)-Avg % foreign born from Demographic table\n",
    " * native_american: double (nullable = true)-Avg % Native American opulation from Demographic table\n",
    " * Asian: double (nullable = true)-Avg % Asian population from Demographic table\n",
    " * hispanic_or_latino: double (nullable = true)-% Avg Hispanic or Latino population per state from Demographic table\n",
    " * Black: double (nullable = true)-% Avg Black population per state from Demographic table\n",
    " * White: double (nullable = true)-% Avg White population per state from Demographic table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I use Apache Spark to get data ready for analytic process. \n",
    "\n",
    "2. The data should be updated quarterly. \n",
    "\n",
    "3. Under the following scenarios, I would approach the problem differently:\n",
    "\n",
    "* I would use Apache spark under HDFS to parelarize the data processing tasks.\n",
    "* And for orchestration I sugest apache airflow. \n",
    "* For user demand as the project get bigger, I sugest a cloud infrastructure(IBM, AWS, GCP or Azure) to  handle the user demand.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
